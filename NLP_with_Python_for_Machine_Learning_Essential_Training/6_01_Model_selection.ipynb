{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Pipeline\n",
    "\n",
    "1. Read in raw text\n",
    "2. Clean text and tokenize\n",
    "3. Feature engineering\n",
    "4. Fit simple model\n",
    "5. Tune hyperparameters and evaluate with GridSearchCV\n",
    "6. Final model selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we've been bending the rules a bit regarding to our vectorizers.  \n",
    "Vectorizers are like models, they should be fit on a training set and only be used to transform the test set. When it transforms the test set, it will only create columns for the words that were in the training set. Any words that appear in the test set but not in the training set will not show up in the vectorized version of the test set. The vectorizer will only recognize words that it saw in the training set.  \n",
    "Up to this moment we've been training the vectorizer on the entire data set, instead of just the training set because it makes things easier with Grid-search and breaking them apart would require an introduction to scikit-learn pipelines.  \n",
    "That's why we're going to tweak that process just a little bit as we go into the final model selection.  \n",
    "  \n",
    "Process\n",
    "1. Split the data into training and test set\n",
    "2. Train vectorizers on training set and use that to transform test set\n",
    "3. Fit best random forest model and best gradient boosting model on training set and predict on test set\n",
    "4. Evaluate results of these two models to select the best one"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in & clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import string\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['body_text', 'body_len', 'punct%']], data['label'], test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089mi</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>020603</th>\n",
       "      <th>...</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtoriu</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>é</th>\n",
       "      <th>ü</th>\n",
       "      <th>üll</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57</td>\n",
       "      <td>24.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138</td>\n",
       "      <td>8.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>145</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.274595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct%         0  008704050406    0089mi  0121  01223585236  \\\n",
       "0        57    24.6  0.0  0.0           0.0  0.000000   0.0          0.0   \n",
       "1        60     5.0  0.0  0.0           0.0  0.000000   0.0          0.0   \n",
       "2        39     7.7  0.0  0.0           0.0  0.000000   0.0          0.0   \n",
       "3       138     8.7  0.0  0.0           0.0  0.000000   0.0          0.0   \n",
       "4       145     2.1  0.0  0.0           0.0  0.274595   0.0          0.0   \n",
       "\n",
       "   01223585334  020603  ...  zhong  zindgi  zoe  zogtoriu  zouk  zyada    é  \\\n",
       "0          0.0     0.0  ...    0.0     0.0  0.0       0.0   0.0    0.0  0.0   \n",
       "1          0.0     0.0  ...    0.0     0.0  0.0       0.0   0.0    0.0  0.0   \n",
       "2          0.0     0.0  ...    0.0     0.0  0.0       0.0   0.0    0.0  0.0   \n",
       "3          0.0     0.0  ...    0.0     0.0  0.0       0.0   0.0    0.0  0.0   \n",
       "4          0.0     0.0  ...    0.0     0.0  0.0       0.0   0.0    0.0  0.0   \n",
       "\n",
       "     ü  üll  〨ud  \n",
       "0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 7217 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "\n",
    "# fit\n",
    "tfidf_vect_fit = tfidf_vect.fit(X_train['body_text'])\n",
    "\n",
    "# transform\n",
    "tfidf_train = tfidf_vect_fit.transform(X_train['body_text'])\n",
    "tfidf_test = tfidf_vect_fit.transform(X_test['body_text'])\n",
    "\n",
    "# create features\n",
    "X_train_vect = pd.concat([\n",
    "    X_train[['body_len', 'punct%']].reset_index(drop=True),\n",
    "    # newly created df comes with a brand new set of indices, but X_train is still keeping the index from the original dataset\n",
    "    # the indices for two dataframes do not match, but the text messages are still in the same order\n",
    "    # so, we can reset indices dropping the old one and now indices of 2 dataframes will match  \n",
    "    pd.DataFrame(tfidf_train.toarray(), columns=tfidf_vect.get_feature_names_out())\n",
    "], axis=1) # what axis we want to concatenate on \n",
    "# (axis=1 means concatenation side by side, axis=0 would stack X_train on top of the tfidf_train)\n",
    "\n",
    "X_test_vect = pd.concat([\n",
    "    X_test[['body_len', 'punct%']].reset_index(drop=True),\n",
    "    pd.DataFrame(tfidf_test.toarray(), columns=tfidf_vect.get_feature_names_out())\n",
    "], axis=1)\n",
    "\n",
    "X_train_vect.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see not we have only 7k words instead of 8k when we fit vectorizer on full dataset instead of just train one.  \n",
    "It tells us that there are around 1k words in the test set that won't be recognized by the vectorizer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 2.4 / Predict time: 0.192 / Precision: 1.0 / Recall: 0.867 / Accuracy: 0.982\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)\n",
    "\n",
    "start = time.time()\n",
    "rf_model = rf.fit(X_train_vect, y_train)\n",
    "end = time.time()\n",
    "fit_time = round((end - start), 3)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = rf_model.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = round((end - start), 3)\n",
    "\n",
    "precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "precision = round(precision, 3)\n",
    "recall = round(recall, 3)\n",
    "accuracy = round((y_pred==y_test).sum() / len(y_pred), 3)\n",
    "\n",
    "print(f'Fit time: {fit_time} / Predict time: {pred_time} / Precision: {precision} / Recall: {recall} / Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 158.618 / Predict time: 0.145 / Precision: 0.883 / Recall: 0.853 / Accuracy: 0.965\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=150, max_depth=15)\n",
    "\n",
    "start = time.time()\n",
    "gb_model = gb.fit(X_train_vect, y_train)\n",
    "end = time.time()\n",
    "fit_time = round((end - start), 3)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = gb_model.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = round((end - start), 3)\n",
    "\n",
    "precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "precision = round(precision, 3)\n",
    "recall = round(recall, 3)\n",
    "accuracy = round((y_pred==y_test).sum() / len(y_pred), 3)\n",
    "\n",
    "print(f'Fit time: {fit_time} / Predict time: {pred_time} / Precision: {precision} / Recall: {recall} / Accuracy: {accuracy}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results we see, that even Gradient Booster takes way longer to fit, it takes less time to predict, so the performance will be better. Precision, recall and accuracy metrics are slightly better for Randlom Forest.  \n",
    "  \n",
    "The final decision should rely on business needs and context. For example for spam filter false positives are very costly and we may need to choose model with better precision, bcz we don't want our spam filter to capture real emails.  \n",
    " \n",
    "Antivirus software for example may be optimized for recall, because not catching actual virus is much worse than false positive.  \n",
    "The performance difference can be a big deal for the systems with potential bottlenecks caused by model activity; "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d2075689bce2225fe1d34c979cffb8c89a74985a811a133c7a2563e25395bbfc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
