{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Boosting** - ensemble method that takes an iterative approach to combining weak learners to create a strong learner by focusing on mistakes of prior iterations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest vs Gradient Boosting\n",
    "\n",
    "Random Forest builds a certain number of fully-grown decision trees simultaneously and then uses a voting method to combine the predictions of each.\n",
    "Instead, Gradient Boosting uses very basic decision trees (like a decision stump) and then it evaluates what it gets right and what it gets wrong on that first tree, and then with the next iteration it places a heavier weight on those observations that it got wrong and it does this over and over again focusing on the examples it doesn't quite understand yet, until it has minimized the error as much as possible.  \n",
    "\n",
    "<img src=\"random_forest_vs_gradient_boosting.png\" width=\"500\">\n",
    "\n",
    "- Both boosting and bagging method include sampling for each different tree that is built, but bagging samples randomly while boosting samples with an increased weight on the ones that it got wrong previously.\n",
    "- Random Forest can train faster because training is done in parallel, and Gradient Boosting is iterative and much longer to train.\n",
    "- Random Forest uses unweighted voting (unweighted average).\n",
    "- Random Forest is faster, but Gradient Boosting is more powerfull and better-performing if tuned properly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros of Gradient Boosting\n",
    "\n",
    "- Extremely powerful\n",
    "- Accepts various types of inputs\n",
    "- Can be used for classification or regression\n",
    "- Outputs feature importance\n",
    "\n",
    "#### Cons of Gradient Boosting\n",
    "\n",
    "- Longer to train (can't parallelize)\n",
    "- More likely to overfit (can get lost pursuing those outliers that don't really represent the overall population)\n",
    "- More difficult to properly tune (more parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d2075689bce2225fe1d34c979cffb8c89a74985a811a133c7a2563e25395bbfc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
