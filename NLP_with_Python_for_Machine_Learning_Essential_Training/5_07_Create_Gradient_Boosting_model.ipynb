{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building ML Classifiers: Create Gradient Boosting model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in & clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import string\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089mi</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>...</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtoriu</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>é</th>\n",
       "      <th>ü</th>\n",
       "      <th>üll</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct%         0  008704050406  0089mi  0121  01223585236  \\\n",
       "0       128     4.7  0.0  0.0           0.0     0.0   0.0          0.0   \n",
       "1        49     4.1  0.0  0.0           0.0     0.0   0.0          0.0   \n",
       "2        62     3.2  0.0  0.0           0.0     0.0   0.0          0.0   \n",
       "3        28     7.1  0.0  0.0           0.0     0.0   0.0          0.0   \n",
       "4       135     4.4  0.0  0.0           0.0     0.0   0.0          0.0   \n",
       "\n",
       "   01223585334  0125698789  ...  zindgi  zoe  zogtoriu  zoom  zouk  zyada  \\\n",
       "0          0.0         0.0  ...     0.0  0.0       0.0   0.0   0.0    0.0   \n",
       "1          0.0         0.0  ...     0.0  0.0       0.0   0.0   0.0    0.0   \n",
       "2          0.0         0.0  ...     0.0  0.0       0.0   0.0   0.0    0.0   \n",
       "3          0.0         0.0  ...     0.0  0.0       0.0   0.0   0.0    0.0   \n",
       "4          0.0         0.0  ...     0.0  0.0       0.0   0.0   0.0    0.0   \n",
       "\n",
       "     é    ü  üll  〨ud  \n",
       "0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 8106 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(data['body_text'])\n",
    "X_tfidf_feat = pd.concat([data['body_len'], data['punct%'], pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vect.get_feature_names_out())], axis=1)\n",
    "X_tfidf_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089mi</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>...</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtoriu</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>é</th>\n",
       "      <th>ü</th>\n",
       "      <th>üll</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct%     0  008704050406  0089mi  0121  01223585236  \\\n",
       "0       128     4.7  0  0             0       0     0            0   \n",
       "1        49     4.1  0  0             0       0     0            0   \n",
       "2        62     3.2  0  0             0       0     0            0   \n",
       "3        28     7.1  0  0             0       0     0            0   \n",
       "4       135     4.4  0  0             0       0     0            0   \n",
       "\n",
       "   01223585334  0125698789  ...  zindgi  zoe  zogtoriu  zoom  zouk  zyada  é  \\\n",
       "0            0           0  ...       0    0         0     0     0      0  0   \n",
       "1            0           0  ...       0    0         0     0     0      0  0   \n",
       "2            0           0  ...       0    0         0     0     0      0  0   \n",
       "3            0           0  ...       0    0         0     0     0      0  0   \n",
       "4            0           0  ...       0    0         0     0     0      0  0   \n",
       "\n",
       "   ü  üll  〨ud  \n",
       "0  0    0    0  \n",
       "1  0    0    0  \n",
       "2  0    0    0  \n",
       "3  0    0    0  \n",
       "4  0    0    0  \n",
       "\n",
       "[5 rows x 8106 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer=clean_text)\n",
    "X_count = count_vect.fit_transform(data['body_text'])\n",
    "X_count_feat = pd.concat([data['body_len'], data['punct%'], pd.DataFrame(X_count.toarray(), columns=count_vect.get_feature_names_out())], axis=1)\n",
    "X_count_feat.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore GradientBoostingClassifier Attributes & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_SUPPORTED_LOSS', '__abstractmethods__', '__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_check_feature_names', '_check_initialized', '_check_n_features', '_check_params', '_clear_state', '_compute_partial_dependence_recursion', '_estimator_type', '_fit_stage', '_fit_stages', '_get_param_names', '_get_tags', '_init_state', '_is_initialized', '_make_estimator', '_more_tags', '_raw_predict', '_raw_predict_init', '_repr_html_', '_repr_html_inner', '_repr_mimebundle_', '_required_parameters', '_resize_state', '_staged_raw_predict', '_validate_data', '_validate_estimator', '_validate_y', '_warn_mae_for_criterion', 'apply', 'decision_function', 'feature_importances_', 'fit', 'get_params', 'n_features_', 'predict', 'predict_log_proba', 'predict_proba', 'score', 'set_params', 'staged_decision_function', 'staged_predict', 'staged_predict_proba']\n"
     ]
    }
   ],
   "source": [
    "print(dir(GradientBoostingClassifier))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributes and methods are almost the same as they are with Random Forest.  \n",
    "  \n",
    "As for default attributes passed in *GrdientBoostingClassifier* when creating its object, `max_depth=3` and `n_estimators=100`. In *RandomForestClassifier* it is None and 10, because it is built with a couple of fully grown trees, whereas GradientBoostingClassifier uses a lot of very basic trees.  \n",
    "\n",
    "Also there is no `n_jobs` parameters which used for parallelizing training in *RandomForestClassifier* (n_jobs=-1).  \n",
    "  \n",
    "Additional parameter `learning_rate=0.1` determines how quickly an algorithm optimizes, but it also has performance implications, because it can cause the model to optimize too quickly, without truly finding the best model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test hyperparameters with Grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_feat, data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "\n",
    "def train_GB(n_est, max_depth, lr):\n",
    "    gb = GradientBoostingClassifier(n_estimators=n_est, max_depth=max_depth, learning_rate=lr)\n",
    "    gb_model = gb.fit(X_train, y_train)\n",
    "    y_pred = gb_model.predict(X_test)\n",
    "    precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "    accuracy = round((y_pred==y_test).sum() / len(y_pred), 3)\n",
    "\n",
    "    test_result = {\n",
    "        'n_estimators': n_est, \n",
    "        'max_depth': max_depth, \n",
    "        'learning_rate': lr, \n",
    "        'precision': precision, \n",
    "        'recall': recall,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "    global results\n",
    "    results = results.append(test_result, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tm0123456789/python-nlp/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tm0123456789/python-nlp/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tm0123456789/python-nlp/.venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for n_est in [50, 100, 150]:\n",
    "    for max_depth in [3, 7, 11, 15]:\n",
    "        for lr in [0.01, 0.1, 1]:\n",
    "            train_GB(n_est, max_depth, lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warnings tell that 3 of the models didn't predict a single text message to be spam, because of that precision could't be calculated and was set to zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Poorly performing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>100.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>150.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>100.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>150.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.643411</td>\n",
       "      <td>0.948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>150.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.620155</td>\n",
       "      <td>0.949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>100.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.635659</td>\n",
       "      <td>0.949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators  max_depth  learning_rate  precision    recall  accuracy\n",
       "0           50.0        3.0           0.01   0.000000  0.000000     0.884\n",
       "3           50.0        7.0           0.01   0.000000  0.000000     0.884\n",
       "6           50.0       11.0           0.01   0.000000  0.000000     0.884\n",
       "9           50.0       15.0           0.01   1.000000  0.007752     0.885\n",
       "12         100.0        3.0           0.01   0.952381  0.465116     0.935\n",
       "24         150.0        3.0           0.01   0.954545  0.488372     0.938\n",
       "15         100.0        7.0           0.01   0.903614  0.581395     0.944\n",
       "30         150.0       11.0           0.01   0.873684  0.643411     0.948\n",
       "27         150.0        7.0           0.01   0.909091  0.620155     0.949\n",
       "21         100.0       15.0           0.01   0.891304  0.635659     0.949"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by=['accuracy'])[0:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the worst performing models have low learning_rate, also a low n_estimators number might have some impact"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Best performing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>150.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.736434</td>\n",
       "      <td>0.964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>100.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.728682</td>\n",
       "      <td>0.963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>150.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.775194</td>\n",
       "      <td>0.961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>150.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.882883</td>\n",
       "      <td>0.759690</td>\n",
       "      <td>0.961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>100.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.882883</td>\n",
       "      <td>0.759690</td>\n",
       "      <td>0.961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>150.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.883929</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.882883</td>\n",
       "      <td>0.759690</td>\n",
       "      <td>0.961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>150.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.728682</td>\n",
       "      <td>0.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>100.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.946809</td>\n",
       "      <td>0.689922</td>\n",
       "      <td>0.960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators  max_depth  learning_rate  precision    recall  accuracy\n",
       "25         150.0        3.0            0.1   0.940594  0.736434     0.964\n",
       "23         100.0       15.0            1.0   0.940000  0.728682     0.963\n",
       "19         100.0       11.0            0.1   0.891892  0.767442     0.962\n",
       "31         150.0       11.0            0.1   0.877193  0.775194     0.961\n",
       "34         150.0       15.0            0.1   0.882883  0.759690     0.961\n",
       "22         100.0       15.0            0.1   0.882883  0.759690     0.961\n",
       "28         150.0        7.0            0.1   0.883929  0.767442     0.961\n",
       "7           50.0       11.0            0.1   0.882883  0.759690     0.961\n",
       "29         150.0        7.0            1.0   0.903846  0.728682     0.960\n",
       "13         100.0        3.0            0.1   0.946809  0.689922     0.960"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by=['accuracy'], ascending=False)[0:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results here the best performing models have learning_rate 0.1 and high number of n_estimators."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Gradient Boosting model performance using Grid-search and Cross-validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Grid-search*: Exhaustively search all parameter combinations in a given grid to determine the best model.  \n",
    "*Cross-validation*: Divide a dataset into k subsets and repeat the holdout method k times where a different subset is used as the holdout set in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to pass best hyperparameters determined in previous step using only Grid-search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>332.295099</td>\n",
       "      <td>4.677119</td>\n",
       "      <td>0.206143</td>\n",
       "      <td>0.054555</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 15, 'n_est...</td>\n",
       "      <td>0.966786</td>\n",
       "      <td>0.977558</td>\n",
       "      <td>0.969452</td>\n",
       "      <td>0.971249</td>\n",
       "      <td>0.966757</td>\n",
       "      <td>0.970360</td>\n",
       "      <td>0.003980</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>199.548763</td>\n",
       "      <td>4.430043</td>\n",
       "      <td>0.274043</td>\n",
       "      <td>0.015876</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 7, 'n_esti...</td>\n",
       "      <td>0.969479</td>\n",
       "      <td>0.979354</td>\n",
       "      <td>0.969452</td>\n",
       "      <td>0.964960</td>\n",
       "      <td>0.966757</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.004980</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>321.780042</td>\n",
       "      <td>16.437967</td>\n",
       "      <td>0.450768</td>\n",
       "      <td>0.107292</td>\n",
       "      <td>0.1</td>\n",
       "      <td>11</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 11, 'n_est...</td>\n",
       "      <td>0.965889</td>\n",
       "      <td>0.977558</td>\n",
       "      <td>0.970350</td>\n",
       "      <td>0.970350</td>\n",
       "      <td>0.964960</td>\n",
       "      <td>0.969821</td>\n",
       "      <td>0.004461</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>260.315121</td>\n",
       "      <td>3.449122</td>\n",
       "      <td>0.345161</td>\n",
       "      <td>0.050520</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 15, 'n_est...</td>\n",
       "      <td>0.963196</td>\n",
       "      <td>0.974865</td>\n",
       "      <td>0.967655</td>\n",
       "      <td>0.968553</td>\n",
       "      <td>0.968553</td>\n",
       "      <td>0.968565</td>\n",
       "      <td>0.003724</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128.354702</td>\n",
       "      <td>0.724140</td>\n",
       "      <td>0.275031</td>\n",
       "      <td>0.018426</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 7, 'n_esti...</td>\n",
       "      <td>0.962298</td>\n",
       "      <td>0.974865</td>\n",
       "      <td>0.966757</td>\n",
       "      <td>0.964960</td>\n",
       "      <td>0.967655</td>\n",
       "      <td>0.967307</td>\n",
       "      <td>0.004199</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "5     332.295099      4.677119         0.206143        0.054555   \n",
       "1     199.548763      4.430043         0.274043        0.015876   \n",
       "3     321.780042     16.437967         0.450768        0.107292   \n",
       "4     260.315121      3.449122         0.345161        0.050520   \n",
       "0     128.354702      0.724140         0.275031        0.018426   \n",
       "\n",
       "  param_learning_rate param_max_depth param_n_estimators  \\\n",
       "5                 0.1              15                150   \n",
       "1                 0.1               7                150   \n",
       "3                 0.1              11                150   \n",
       "4                 0.1              15                100   \n",
       "0                 0.1               7                100   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "5  {'learning_rate': 0.1, 'max_depth': 15, 'n_est...           0.966786   \n",
       "1  {'learning_rate': 0.1, 'max_depth': 7, 'n_esti...           0.969479   \n",
       "3  {'learning_rate': 0.1, 'max_depth': 11, 'n_est...           0.965889   \n",
       "4  {'learning_rate': 0.1, 'max_depth': 15, 'n_est...           0.963196   \n",
       "0  {'learning_rate': 0.1, 'max_depth': 7, 'n_esti...           0.962298   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "5           0.977558           0.969452           0.971249           0.966757   \n",
       "1           0.979354           0.969452           0.964960           0.966757   \n",
       "3           0.977558           0.970350           0.970350           0.964960   \n",
       "4           0.974865           0.967655           0.968553           0.968553   \n",
       "0           0.974865           0.966757           0.964960           0.967655   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "5         0.970360        0.003980                1  \n",
       "1         0.970000        0.004980                2  \n",
       "3         0.969821        0.004461                3  \n",
       "4         0.968565        0.003724                4  \n",
       "0         0.967307        0.004199                5  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "param = {\n",
    "    'n_estimators': [100, 150],\n",
    "    'max_depth': [7, 11, 15],\n",
    "    'learning_rate': [0.1] # default value, could be skipped\n",
    "}\n",
    "gs = GridSearchCV(gb, param, cv=5, n_jobs=-1)\n",
    "# n_jobs=-1 means that we're going to train models on different subsets and parameter settings in parallel,\n",
    "# it is impossible to train sub-models in the same Gradient Boosting model, \n",
    "# because they are trained iteratively and each iteration depends on prior iteration\n",
    "\n",
    "tfidf_cv_fit = gs.fit(X_tfidf_feat, data['label'])\n",
    "pd.DataFrame(tfidf_cv_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the `mean_fit_time` is significantly larger than for Random Forest where the most consuming model took around 30 seconds to fit.  \n",
    "  \n",
    "All Gradient Boosting models getting perfect `mean_train_score` of 1.0 on the training set (not displayed). If the model is overfitting ot the point of just memorizing the training set that's bad because it won't do well generalizing to the test set.  \n",
    "That's why we really care only about the test score, that's what tells us whether model can generalize to data that it was not trained on.  \n",
    "  \n",
    "Best model here uses max_depth 15 and 150 estimators."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>294.314402</td>\n",
       "      <td>4.696841</td>\n",
       "      <td>0.350255</td>\n",
       "      <td>0.059377</td>\n",
       "      <td>0.1</td>\n",
       "      <td>11</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 11, 'n_est...</td>\n",
       "      <td>0.965889</td>\n",
       "      <td>0.976661</td>\n",
       "      <td>0.969452</td>\n",
       "      <td>0.964960</td>\n",
       "      <td>0.973046</td>\n",
       "      <td>0.970001</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>238.108096</td>\n",
       "      <td>11.349250</td>\n",
       "      <td>0.303271</td>\n",
       "      <td>0.018349</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 15, 'n_est...</td>\n",
       "      <td>0.962298</td>\n",
       "      <td>0.977558</td>\n",
       "      <td>0.968553</td>\n",
       "      <td>0.964061</td>\n",
       "      <td>0.971249</td>\n",
       "      <td>0.968744</td>\n",
       "      <td>0.005431</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>316.758846</td>\n",
       "      <td>3.951857</td>\n",
       "      <td>0.208862</td>\n",
       "      <td>0.025770</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 15, 'n_est...</td>\n",
       "      <td>0.964093</td>\n",
       "      <td>0.976661</td>\n",
       "      <td>0.969452</td>\n",
       "      <td>0.962264</td>\n",
       "      <td>0.971249</td>\n",
       "      <td>0.968744</td>\n",
       "      <td>0.005159</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>205.249200</td>\n",
       "      <td>3.941392</td>\n",
       "      <td>0.313091</td>\n",
       "      <td>0.036486</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 7, 'n_esti...</td>\n",
       "      <td>0.964991</td>\n",
       "      <td>0.979354</td>\n",
       "      <td>0.971249</td>\n",
       "      <td>0.963163</td>\n",
       "      <td>0.964960</td>\n",
       "      <td>0.968743</td>\n",
       "      <td>0.005973</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>195.556990</td>\n",
       "      <td>4.022713</td>\n",
       "      <td>0.287574</td>\n",
       "      <td>0.018015</td>\n",
       "      <td>0.1</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 11, 'n_est...</td>\n",
       "      <td>0.964991</td>\n",
       "      <td>0.974865</td>\n",
       "      <td>0.965858</td>\n",
       "      <td>0.962264</td>\n",
       "      <td>0.969452</td>\n",
       "      <td>0.967486</td>\n",
       "      <td>0.004347</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "3     294.314402      4.696841         0.350255        0.059377   \n",
       "4     238.108096     11.349250         0.303271        0.018349   \n",
       "5     316.758846      3.951857         0.208862        0.025770   \n",
       "1     205.249200      3.941392         0.313091        0.036486   \n",
       "2     195.556990      4.022713         0.287574        0.018015   \n",
       "\n",
       "  param_learning_rate param_max_depth param_n_estimators  \\\n",
       "3                 0.1              11                150   \n",
       "4                 0.1              15                100   \n",
       "5                 0.1              15                150   \n",
       "1                 0.1               7                150   \n",
       "2                 0.1              11                100   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "3  {'learning_rate': 0.1, 'max_depth': 11, 'n_est...           0.965889   \n",
       "4  {'learning_rate': 0.1, 'max_depth': 15, 'n_est...           0.962298   \n",
       "5  {'learning_rate': 0.1, 'max_depth': 15, 'n_est...           0.964093   \n",
       "1  {'learning_rate': 0.1, 'max_depth': 7, 'n_esti...           0.964991   \n",
       "2  {'learning_rate': 0.1, 'max_depth': 11, 'n_est...           0.964991   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "3           0.976661           0.969452           0.964960           0.973046   \n",
       "4           0.977558           0.968553           0.964061           0.971249   \n",
       "5           0.976661           0.969452           0.962264           0.971249   \n",
       "1           0.979354           0.971249           0.963163           0.964960   \n",
       "2           0.974865           0.965858           0.962264           0.969452   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "3         0.970001        0.004388                1  \n",
       "4         0.968744        0.005431                2  \n",
       "5         0.968744        0.005159                3  \n",
       "1         0.968743        0.005973                4  \n",
       "2         0.967486        0.004347                5  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "param= {\n",
    "    'n_estimators': [100, 150],\n",
    "    'max_depth': [7, 11, 15],\n",
    "    'learning_rate': [0.1] # default value, could be skipped\n",
    "}\n",
    "gs = GridSearchCV(gb, param, cv=5, n_jobs=-1)\n",
    "# n_jobs=-1 means that we're going to train models on different subsets and parameter settings in parallel,\n",
    "# it is impossible to train sub-models in the same Gradient Boosting model, \n",
    "# because they are trained iteratively and each iteration depends on prior iteration\n",
    "\n",
    "count_cv_fit = gs.fit(X_count_feat, data['label'])\n",
    "pd.DataFrame(count_cv_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of using count vectorizer are very similar to tfidf (best model with max_depth=11 and n_estimators=150), but test score is slightly lower."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b11f6fa748e78b350860d1b1d59f7550718e3f8acbe8713cb1893ff1fa0b924"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
