{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data using PlainCorpusReader\n",
    "\n",
    "Nltk supports multiple CorpusReaders depending upon the type of data source. Details available in http://www.nltk.org/howto/corpus.html  \n",
    "  \n",
    "The PlaintextCorpusReader is used to read a list of text files under a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You'll learn NLP.\n",
      "\n",
      "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
      "\n",
      "Thanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation, NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike. NLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project.\n",
      "\n",
      "NLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,” and “an amazing library to play with natural language.”\n",
      "\n",
      "Natural Language Processing with Python provides a practical introduction to programming for language processing. Written by the creators of NLTK, it guides the reader through the fundamentals of writing Python programs, working with corpora, categorizing text, analyzing linguistic structure, and more.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "# Read the file into a corpus. The same command can read an entire directory\n",
    "corpus = PlaintextCorpusReader(os.getcwd(), \"data.txt\")\n",
    "\n",
    "# Print raw contents of the corpus\n",
    "print(corpus.raw())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each file in directory becomes a single file id. The contents of the file together constitue a corpus.  \n",
    "Data is then split into paragraphs, sentences and tokens automatically, while the corpus is read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files: ['data.txt']\n",
      "\n",
      "Total paragraphs: 5\n",
      "\n",
      "Total sentences: 9\n",
      "\n",
      "The first sentence: ['You', \"'\", 'll', 'learn', 'NLP', '.']\n",
      "\n",
      "Words: ['You', \"'\", 'll', 'learn', 'NLP', '.', 'NLTK', 'is', ...]\n"
     ]
    }
   ],
   "source": [
    "# Extract file ids, paragraphs, sentences and words from the corpus\n",
    "print(f'Files: {corpus.fileids()}\\n')\n",
    "\n",
    "paragraphs = corpus.paras()\n",
    "print(f'Total paragraphs: {len(corpus.paras())}\\n')\n",
    "\n",
    "sentences = corpus.sents()\n",
    "print(f'Total sentences: {len(sentences)}\\n')\n",
    "print(f'The first sentence: {sentences[0]}\\n')\n",
    "\n",
    "print(f'Words: {corpus.words()}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Corpus\n",
    "\n",
    "The NLTK library provides a number of functions to analyze the distributions and aggregates for data in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in the corpus :  [(',', 27), ('.', 8), ('and', 8), ('for', 7), ('NLTK', 6), ('a', 6), ('to', 6), ('with', 5), ('-', 5), ('is', 4)]\n",
      "\n",
      " Distribution for \"Spark\" :  None\n"
     ]
    }
   ],
   "source": [
    "#Find the frequency distribution of words in the corpus\n",
    "course_freq_dist=nltk.FreqDist(corpus.words())\n",
    "\n",
    "#Print most commonly used words\n",
    "print(\"Top 10 words in the corpus : \", course_freq_dist.most_common(10))\n",
    "\n",
    "#find the distribution for a specific word\n",
    "print(\"\\n Distribution for \\\"Spark\\\" : \",course_freq_dist.get(\"Spark\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d2075689bce2225fe1d34c979cffb8c89a74985a811a133c7a2563e25395bbfc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
