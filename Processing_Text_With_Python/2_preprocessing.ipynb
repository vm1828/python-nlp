{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Data Cleaning\n",
    "\n",
    "**Tokenization** - converting a text string into individual tokens (words, terms, symbols or other meaningful elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list: ['You', \"'ll\", 'learn', 'NLP', '.', 'NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.', 'It', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', 'such', 'as', 'WordNet', ',', 'along', 'with', 'a', 'suite']\n",
      "\n",
      "Total tokens: 217\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "with open('data.txt', 'rt') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "tokens_with_punctuation = nltk.word_tokenize(raw_text)\n",
    "print(f'Token list: {tokens_with_punctuation[:40]}\\n')\n",
    "print(f'Total tokens: {len(tokens_with_punctuation)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Text data cleaning:_\n",
    "\n",
    "- Formatting and standartization (e.g. dates, language)\n",
    "- Remove punctuation\n",
    "- Remove or convert abbreviations to full form\n",
    "- Case conversion\n",
    "- Remove elements like hashtags\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation, Convert to lowercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list: ['you', \"'ll\", 'learn', 'nlp', 'nltk', 'is', 'a', 'leading', 'platform', 'for', 'building', 'python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', 'it', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', 'such', 'as', 'wordnet', 'along', 'with', 'a', 'suite', 'of', 'text', 'processing']\n",
      "\n",
      "Total tokens: 177\n"
     ]
    }
   ],
   "source": [
    "# the best way to get rid of punctuation, preserving contractions and compound words with hyphens\n",
    "tokens = [token.lower()\n",
    "          for token in tokens_with_punctuation if nltk.tokenize.punkt.PunktToken(token).is_non_punct]\n",
    "\n",
    "print(f'Token list: {tokens[:40]}\\n')\n",
    "print(f'Total tokens: {len(tokens)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords\n",
    "\n",
    "**Stopwords** - a group of words that carry on meaning by themselves (e.g. 'in', 'and', 'the', 'which')  \n",
    "Stopwords not required for analytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list: [\"'ll\", 'learn', 'nlp', 'nltk', 'leading', 'platform', 'building', 'python', 'programs', 'work', 'human', 'language', 'data', 'provides', 'easy-to-use', 'interfaces', '50', 'corpora', 'lexical', 'resources', 'wordnet', 'along', 'suite', 'text', 'processing', 'libraries', 'classification', 'tokenization', 'stemming', 'tagging', 'parsing', 'semantic', 'reasoning', 'wrappers', 'industrial-strength', 'nlp', 'libraries', 'active', 'discussion', 'forum']\n",
      "\n",
      "Total tokens: 119\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "print(f'Token list: {tokens[:40]}\\n')\n",
    "print(f'Total tokens: {len(tokens)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming/Lemmatization\n",
    "\n",
    "Lemmatization uses a dictionary to match words to their root word.  \n",
    "It is more expensive operation than stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming: mean, mean\n",
      "Lemmatization: meaning, meanness\n"
     ]
    }
   ],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "print(f\"Stemming: {ps.stem('meanness')}, {ps.stem('meaning')}\")\n",
    "print(f\"Lemmatization: {wn.lemmatize('meaning')}, {wn.lemmatize('meanness')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list: [\"'ll\", 'learn', 'nlp', 'nltk', 'lead', 'platform', 'build', 'python', 'program', 'work', 'human', 'languag', 'data', 'provid', 'easy-to-us', 'interfac', '50', 'corpora', 'lexic', 'resourc', 'wordnet', 'along', 'suit', 'text', 'process', 'librari', 'classif', 'token', 'stem', 'tag', 'pars', 'semant', 'reason', 'wrapper', 'industrial-strength', 'nlp', 'librari', 'activ', 'discuss', 'forum']\n",
      "\n",
      "Total tokens: 119\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stem_tokens = [stemmer.stem(token) for token in tokens]\n",
    "print(f'Token list: {stem_tokens[:40]}\\n')\n",
    "print(f'Total tokens: {len(stem_tokens)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token list: [\"'ll\", 'learn', 'nlp', 'nltk', 'leading', 'platform', 'building', 'python', 'program', 'work', 'human', 'language', 'data', 'provides', 'easy-to-use', 'interface', '50', 'corpus', 'lexical', 'resource', 'wordnet', 'along', 'suite', 'text', 'processing', 'library', 'classification', 'tokenization', 'stemming', 'tagging', 'parsing', 'semantic', 'reasoning', 'wrapper', 'industrial-strength', 'nlp', 'library', 'active', 'discussion', 'forum']\n",
      "\n",
      "Total tokens: 119\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lem_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "print(f'Token list: {lem_tokens[:40]}\\n')\n",
    "print(f'Total tokens: {len(lem_tokens)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams\n",
    "\n",
    "N-grams is a sequence of n items in text sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common bigrams: [(('python', 'program'), 2), (('computational', 'linguistics'), 2), (('language', 'processing'), 2), ((\"'ll\", 'learn'), 1), (('learn', 'nlp'), 1)]\n",
      "Most common trigrams: [((\"'ll\", 'learn', 'nlp'), 1), (('learn', 'nlp', 'nltk'), 1), (('nlp', 'nltk', 'leading'), 1), (('nltk', 'leading', 'platform'), 1), (('leading', 'platform', 'building'), 1)]\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "bigrams = ngrams(lem_tokens, 2)\n",
    "print(f\"Most common bigrams: {Counter(bigrams).most_common(5)}\")\n",
    "\n",
    "trigrams = ngrams(lem_tokens, 3)\n",
    "print(f\"Most common trigrams: {Counter(trigrams).most_common(5)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts-of-Speech (POS) Tagging\n",
    "\n",
    "- POS tagging involves identifying the part of speech for each word in a corpus\n",
    "- Used for entity recognition, filtering and sentiment analysis\n",
    "\n",
    "| Word   | POS | Description           |\n",
    "| ------ | --- | --------------------- |\n",
    "| Man    | NN  | Noun                  |\n",
    "| Engage | VBP | Verb Singular Present |\n",
    "| Top    | JJ  | Adjective             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'ll\", 'MD'),\n",
       " ('learn', 'VB'),\n",
       " ('nlp', 'JJ'),\n",
       " ('nltk', 'JJ'),\n",
       " ('leading', 'VBG'),\n",
       " ('platform', 'NN'),\n",
       " ('building', 'NN'),\n",
       " ('python', 'NN'),\n",
       " ('program', 'NN'),\n",
       " ('work', 'NN')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# tag and print first 10 tokens\n",
    "nltk.pos_tag(lem_tokens)[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d2075689bce2225fe1d34c979cffb8c89a74985a811a133c7a2563e25395bbfc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
